\section{Network Function Chaining Infrastructure based on Virtualization Technologies} 
\subsection{Network Function Virtualization}
Traditionally carrier network has provided network functions (NF) like P-GW and S-GW on proprietary, dedicated hardware. However these hardware-based NFs have been being replaced by software, because of its high installation cost and relatively short lifetime. Furthermore flexible deployment of NF is required for the upcoming 5G wireless and Internet of Things (IoT). Since the software-based NFs are decoupled from the platform hardware, it is easy to newly deploy NF or withhold NF. 

Implementation of NF in software is realized by virtualization technology. This is called Network Function Virtualization (NFV). Virtualization technology is categorized into virtual machine (VM)-based and container-based technology. The former is full virtualization which creates virtual instances of hardware like CPU, RAM and network interface for every VM. Therefore a VM runs on a full copy on an operation system (OS) and virtual emulation of all needed hardware. A NF that runs as a VM is for these reasons resource consuming. The latter uses container-based technology represented by Docker\cite{Docker}. Container is a loosely isolated environment which runs on the kernel of host node. Isolation is done using five namespaces: process, network, mount, hostname, and shared memory. In contrast to VM, containers are light-weighted because each of them does not require virtualized resources.  

There are two main advantages in the virtualization technology. The first advantage of using them is the portability of NFs. Any NF that runs on a VM or container instance can operate on any host that supports the virtualization technologies. In case of KVM-based\cite{KVM} VM, a configuration file written by XML file and image file (raw or qcow format) and in case of Docker-based container, with Dockerfile (configuration file) and Docker image (disk image file) the same instance can be build. The second advantage is that the isolation is provided between instances (VMs or Containers). This enables the CPU and memory resources to be allocated to the instance as required. And the instance cannot take up more resources than allocated. 

Because of those advantages, VM and container are used very commonly in NFV. A host can have multiple NFs (VMs or containers), and the traffic between NFs are exchangeable with software switches. Combined with software-defined network (SDN), traffic can be managed to go through specific NFs in other hosts. Either by chaining NFs in a single host or between several hosts, a network service is created. 

<In this paper, I propose a novel NF chaining mechanism inside of the kernel in a single host. >

\begin{comment}
	\subsection{Disadvantage of VM-based/Docker-based NFV}
The main disadvantage is the packet forwarding: Each packet must be transmitted between NIC and instances (VM or Container) causing throughput bottleneck. I describe the detail of packet forwarding in VM-based and container-based technology, respectively in section 1.1.4 and 1.1.5. 
\end{comment}

\subsection{Packet forwarding in virtualized nodes}
In this chapter the detail of packet forwarding between the NIC of host and virtualized nodes like VM or container is explained. 

In case of VM-based NFV, packet forwarding consists of three architectures, packet I/O, virtual network I/O and virtual switch. 
\begin{enumerate}
	\item Packet I/O: 
	 Packet I/O is the part where packets pass through between physical NIC and user space. The default of Linux is NAPI (New API)\cite{NAPI}, but since the network stack of NAPI is considered redundant, there are acceleration technology like Intel DPDK\cite{DPDK}, XDP (eXpress Data Path)\cite{XDP} and Netmap\cite{Netmap} to bypass network stack. DPDK provides libraries that enables user space process to directly access physical NIC's packet buffer. XDP and Netmap use memory mapping between NIC's packet buffer and user space's shared memory region, to give user space application direct access to packets. 
	\item Virtual network I/O: 
	Virtual network I/O mechanism is used for networking VMs and the underlying hypervisor. In KVM, each host (VM) has virtio\cite{virtio} buffer in the guest host where outgoing or incoming packets are stored. For the connection of the virtio buffer and the socket buffer in the hypervisor host, TAP interface is used. TAP interface is a software device with two endpoints: one is a NIC and the other one is a file descriptor (fd). KVM process in the hypervisor sends traffic to the fd endpoint of the TAP device. The traffic then arrives at the NIC endpoint which can be connected to other NIC or TAP interface using a virtual switch. Finally the packets are written to the virtio buffer of the host. The reversed path (traffic from host to hypervisor) is vice versa. Since writing to the virtio buffer is done by KVM process in user space, it involves system calls every time. To reduce the overhead by system call, vhost-net\cite{vhost-net} is available in KVM. With vhost-net enabled, kernel thread directly writes to the virtio buffer from TAP device, in stead of KVM process in the user space. 
	\item Virtual switch: 
	Virtual switch is used to forward traffic to the right port as mentioned above. Open vSwitch (OVS)\cite{OVS}, which supports OpenFlow\cite{OpenFlow} is representative among others. OVS uses flow table which has match part and action part. Match part is set of rules to filter packet and can distinguish not only 5 tuples, but also VLAN tags or incoming port, etc. Action part can be drop, modify or transmit packet. When OVS works with virtual network I/O to realize NF chaining, the action should be transfer to specific Layer-2 (L2) address port. 
\end{enumerate}

In case of container-based NFV, packet forwarding is organized as follows. 
By default, Docker uses local private virtual network in the host. It creates virtual bridge and allocates subnet from one of the private address blocks. Each container gets allocated virtual ethernet device which is attached to the interface. In this way, containers in the same subnet can communicate, if they are on the same host. 

Kubernetes project\cite{Kubernetes} makes communication between containers on different hosts possible. Instead of allocating private IP address, Kubernetes gives each pod (a group of containers) a global IP address, allowing NAT-less, flat address space. Either in the normal networking or Kubernetes's networking, network stack in the host is used to reach containers. 

\section{Overhead of NFCI based on Virtualization Technologies}
\subsection{Memory copy}
The recently dominating trend in NFV is to use VM or Docker container to create Virtualized Network Function (VNF). However both of the technologies are not originally built in purpose of achieving high performance in heavy load network. 
Various kinds of efforts have been made to reduce the overhead of packet forwarding as mentioned in Sec. 1.1.3. For example, Netmap maps packet buffer into a memory region which user space process can directly access (zero-copy). But for the interaction with VM, software switch VALE\cite{VALE} which is supported on Netmap copies packets from the memory region to the guest's virtio buffer. Similary in case of DPDK + OVS, memory copy is needed to deliver traffic to VM. I believe that unnecessary memory copy should be avoided not to make possible bottleneck of data forwarding in NF chaining. 

Figure \ref{fig: kvm+ovs} shows the path of the traffic in a VM-based NFV instance with KVM as virtual network I/O and OVS as virtual switch. With NAPI, traffic go through network stack (packet I/O) of the host, bridging by OVS, get input to virtio buffer of the guest and passed again through network stack in the guest. Even when bypassing packet I/O with DPDK or Netmap, traffic has to proceed unnecessary layers in order to be processed in a NF. 

Figure \ref{fig: docker} shows the path of the traffic when containers are used to make NFs and Docker Engine such as Kubernetes supports networking between them. Since container uses host's OS, traffic does not need to pass network stack once more inside the container, as in the case of VM-based node. But the path all the way up from NIC to user space and forwarding data in Docker Engine is yet not optimized for a truly high performance NFV node. 

\subsection{Resource Consumption}
In case of VM-based NFV, a NF consists of one or multiple VMs. To launch a VM, virtual instances such as CPU, memory and network I/O must be created. Since virtual instances are allocated for each of the VM, when managing several VMs it consumes lot of hardware resources. 

\begin{figure}[t]
	\begin{minipage}{0.5\hsize}
		\begin{center}
			\includegraphics[width=85mm]{pics/KVM+OVS.pdf}
		\end{center}
		\caption{Traffic in VM-based NFV node}
		\label{fig: kvm+ovs}
	\end{minipage}	
	\begin{minipage}{0.5\hsize}
		\begin{center}
			\includegraphics[width=85mm]{pics/Docker.pdf}
		\end{center}
		\caption{Traffic in Container-based NFV node}
		\label{fig: docker}
	\end{minipage}	
\end{figure}

\begin{figure*}
	\centering
	\includegraphics[width=95mm]{pics/path_kernel.pdf}
	\caption{Traffic in Kernel-based NFV node}
	\label{fig: path_kernel}
\end{figure*}

\section{Proposal: Lightweight NFC mechanism in Linux Kernel}
Because of the above two reasons, overheads in data path and resource consumption, I propose Kernel-based NFCI. Its aim is to realize lightweight NF chaining as much as possible in the kernel space on a single host. Simple and low load NFs should be executed in the kernel and NFs that require complicated tasks (e.g., tasks using encryption libraries) have to be put in the user space. The scope of this paper is the chaining mechanism for former NFs inside the kernel.

NF consists of one or more kernel modules. In order to integrate NF in network stack and manage traffic flow, I use Netfilter subsystem in Linux. Netfilter is a set of hooks inside the kernel that allows kernel modules to register its functions. A registered function is called for every packet that traverses respective hook within the network stack. The NF chaining takes place in the first hook before the routing decision. 

As Figure \ref{fig: path_kernel} shows, in my kernel-based NFCI node, NFs in kernel can directly access to the SKB because they are all in the same memory region. Hence there is no need of memory copy. When a NF cannot be completely executed in the kernel, only the simple function is placed in the kernel and scheduled by the well-tuned scheduling algorithm in Linux kernel, e.g., Completely Fair Scheduling (CFS)\cite{CFS}. The rest must be put in user space by a virtualization technology to be targeted for resource isolation. Because only complicated function is virtualized, overall resource consumption should be also decreased significantly. 

We think that certain amount of NFs consists of fairly simple functions, e.g., hash-based signature comparison and counting the number of specific flows,  and lots of them are feasible in the kernel. By providing a chaining mechanism for lightweight NFs in the kernel, overheads discussed so far will be largely eliminated. 

The structure of the thesis is as follows. Related work is introduced in Chapter 2. In Chapter 3, the overall picture of the Kernel-based NFCI is described. Chapter 4 describes the architecture of the Kernel-based NFCI. In Chapter 5, the detail of the implementation is explained. Results of the evaluations are shown in Chapter 5. And finally, I conclude this thesis in Chapter 6. 











